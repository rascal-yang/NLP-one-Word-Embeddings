{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于读取语料库文件并将单词转化为对应的索引\n",
    "def read_corpus(file_path, vocab_size):\n",
    "    # 打开指定文件路径的语料库文件，并将内容按空格分割为单词列表\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        words = file.read().split()\n",
    "\n",
    "    # 初始化一个计数列表，用于统计每个单词出现的次数，并将未知词标注为'UNK'\n",
    "    count = [['UNK', -1]]\n",
    "    # 使用 Counter 统计单词出现的次数，并取出现次数最多的前 vocab_size - 1 个单词\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    # 创建一个字典，将单词映射为索引\n",
    "    word_index = dict()\n",
    "    for word, _ in count:\n",
    "        word_index[word] = len(word_index)\n",
    "\n",
    "    # 将所有单词转化为对应的索引存储在 data 列表中，并统计未知词的数量\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            index = word_index[word]\n",
    "        else:\n",
    "            index = 0  # 将未知词标记为索引0，对应'UNK'\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "\n",
    "    # 更新计数列表中'UNK'的出现次数\n",
    "    count[0][1] = unk_count\n",
    "\n",
    "    # 创建一个字典，将索引映射为单词\n",
    "    index_word = dict(zip(word_index.values(), word_index.keys()))\n",
    "\n",
    "    # 返回转化后的单词索引列表、计数列表、索引到单词的映射字典和单词到索引的映射字典\n",
    "    return data, count, index_word, word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, count, index_word, word_index = read_corpus('training.txt', 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampling(data, count):\n",
    "    '''下采样 降低在训练模型时对高频词的过度关注'''\n",
    "    # 统计每个单词出现的次数\n",
    "    count = [ele[1] for ele in count]\n",
    "    # 计算每个单词出现的概率\n",
    "    frequency = np.array(count) / sum(count)\n",
    "\n",
    "    # 计算每个单词调整以后的概率，并存储在 P 字典中\n",
    "    P = dict()\n",
    "    for idx, x in enumerate(frequency):\n",
    "        y = (math.sqrt(x / 0.001) + 1) * 0.001 / x\n",
    "        P[idx] = y\n",
    "\n",
    "    # 对单词列表进行下采样，并将下采样后的单词存储在 subsampled_data 列表中\n",
    "    subsampled_data = list()\n",
    "    for word in data:\n",
    "        if random.random() < P[word]:\n",
    "            subsampled_data.append(word)\n",
    "\n",
    "    # 返回下采样后的单词列表\n",
    "    return subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = subsampling(data, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sample_table(count, table_size=1e8):\n",
    "    '''初始化采样表'''\n",
    "    # 统计每个单词出现的次数\n",
    "    count = [ele[1] for ele in count]\n",
    "    # 计算每个单词出现的概率的 0.75 次方\n",
    "    pow_frequency = np.array(count)**0.75\n",
    "    # 计算概率的和\n",
    "    power = sum(pow_frequency)\n",
    "    # 计算每个单词在采样表中出现的次数\n",
    "    ratio = pow_frequency / power\n",
    "    count = np.round(ratio * table_size)\n",
    "\n",
    "    # 创建采样表\n",
    "    sample_table = []\n",
    "    for idx, x in enumerate(count):\n",
    "        sample_table += [idx] * int(x)\n",
    "\n",
    "    # 将采样表转化为 numpy 数组，并返回\n",
    "    return np.array(sample_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_table = init_sample_table(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于生成训练批次数据\n",
    "def generate_batch(train_data, sample_table, neg_sample_num, window_size, batch_size):\n",
    "    # 将训练数据赋值给变量 data\n",
    "    data = train_data\n",
    "\n",
    "    # 初始化全局变量 data_index，表示当前处理的数据索引\n",
    "    global data_index\n",
    "\n",
    "    # 计算上下文窗口的大小\n",
    "    span = 2 * window_size + 1\n",
    "\n",
    "    # 初始化上下文和标签数组\n",
    "    context = np.ndarray(shape=(batch_size, 2 * window_size), dtype=np.int64)\n",
    "    labels = np.ndarray(shape=(batch_size), dtype=np.int64)\n",
    "\n",
    "    # 如果当前数据索引加上窗口大小超出了数据长度，则重新从数据开头开始\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "\n",
    "    # 从数据中提取当前窗口的数据\n",
    "    buffer = data[data_index:data_index + span]\n",
    "    pos_u = []\n",
    "    pos_v = []\n",
    "\n",
    "    # 遍历每个批次\n",
    "    for i in range(batch_size):\n",
    "        # 更新数据索引\n",
    "        data_index += 1\n",
    "        # 获取上下文单词索引和标签单词索引\n",
    "        context[i, :] = buffer[:window_size] + buffer[window_size + 1:]\n",
    "        labels[i] = buffer[window_size]\n",
    "\n",
    "        # 如果当前数据索引加上窗口大小超出了数据长度，则重新初始化数据索引和缓冲区\n",
    "        if data_index + span > len(data):\n",
    "            buffer[:] = data[:span]\n",
    "            data_index = 0\n",
    "        else:\n",
    "            # 更新缓冲区为下一个窗口数据\n",
    "            buffer = data[data_index:data_index + span]\n",
    "\n",
    "        # 构建正样本对\n",
    "        for j in range(span - 1):\n",
    "            pos_u.append(labels[i])\n",
    "            pos_v.append(context[i, j])\n",
    "\n",
    "    # 从采样表中随机选择负样本\n",
    "    neg_v = np.random.choice(sample_table, size=(batch_size * 2 * window_size, neg_sample_num))\n",
    "\n",
    "    # 返回正样本和负样本数组\n",
    "    return np.array(pos_u), np.array(pos_v), neg_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个 Word2Vec 类，用于训练 Word2Vec 模型\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, vector_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        # 初始化词汇表大小和词向量维度\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "        # 初始化目标词和上下文词的嵌入层\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, vector_size, sparse=True)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, vector_size, sparse=True)\n",
    "        # 初始化嵌入层权重\n",
    "        self.init_emb()\n",
    "\n",
    "    # 初始化嵌入层权重的方法\n",
    "    def init_emb(self):\n",
    "        # 设置初始化范围\n",
    "        initrange = 0.5 / self.vector_size\n",
    "        # 初始化目标词的嵌入权重\n",
    "        self.target_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        # 初始化上下文词的嵌入权重\n",
    "        self.context_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    # 前向传播方法，计算损失函数\n",
    "    def forward(self, u_pos, v_pos, v_neg, batch_size):\n",
    "        # 获取目标词和上下文词的嵌入表示\n",
    "        embed_u = self.target_embeddings(u_pos)\n",
    "        embed_v = self.context_embeddings(v_pos)\n",
    "\n",
    "        # 计算正样本得分\n",
    "        score = torch.mul(embed_u, embed_v)\n",
    "        score = torch.sum(score, dim=1)\n",
    "        log_target = torch.log(torch.sigmoid(score)).squeeze()\n",
    "\n",
    "        # 获取负样本的上下文词的嵌入表示\n",
    "        neg_embed_v = self.context_embeddings(v_neg)\n",
    "\n",
    "        # 计算负样本得分\n",
    "        neg_score = torch.bmm(neg_embed_v, embed_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.sum(neg_score, dim=1)\n",
    "        sum_log_sampled = torch.log(torch.sigmoid(-1 * neg_score)).squeeze()\n",
    "\n",
    "        # 计算损失函数\n",
    "        loss = log_target + sum_log_sampled\n",
    "\n",
    "        # 返回平均损失\n",
    "        return -1 * loss.sum() / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个训练函数，用于训练 Word2Vec 模型\n",
    "def train(train_data, vocabulary_size, embedding_dim, epoch_num, batch_size, window_size, neg_sample_num):\n",
    "    # 初始化 Word2Vec 模型\n",
    "    model = Word2Vec(vocabulary_size, embedding_dim)\n",
    "    # 如果 GPU 可用，则将模型移动到 GPU 上\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    # 定义优化器\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.2)\n",
    "    \n",
    "    # 计算总的批次数\n",
    "    total_batches = len(train_data) // batch_size\n",
    "    \n",
    "    # 遍历每个 epoch\n",
    "    for epoch in range(epoch_num):\n",
    "        batch_num = 0\n",
    "\n",
    "        # 使用 tqdm 包装 while 循环，用于显示进度条\n",
    "        with tqdm(total=total_batches,  desc=f\"Epoch {epoch}\") as pbar:\n",
    "            i = 0\n",
    "            epoch_loss = 0.0  # 用于累积每个 epoch 的 loss\n",
    "            while i < total_batches:\n",
    "                # 生成训练批次数据\n",
    "                pos_u, pos_v, neg_v = generate_batch(train_data, sample_table, neg_sample_num, window_size, batch_size)\n",
    "\n",
    "                # 将数据转换为 PyTorch 张量，并将其移动到 GPU 上\n",
    "                pos_u = Variable(torch.LongTensor(pos_u))\n",
    "                pos_v = Variable(torch.LongTensor(pos_v))\n",
    "                neg_v = Variable(torch.LongTensor(neg_v))\n",
    "                if torch.cuda.is_available():\n",
    "                    pos_u = pos_u.cuda()\n",
    "                    pos_v = pos_v.cuda()\n",
    "                    neg_v = neg_v.cuda()\n",
    "\n",
    "                # 将梯度清零，计算损失函数，进行反向传播，更新模型参数\n",
    "                optimizer.zero_grad()\n",
    "                loss = model(pos_u, pos_v, neg_v, batch_size)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_num += 1\n",
    "                \n",
    "                # 每 30000 个批次保存一次模型\n",
    "                if batch_num % 30000 == 0:\n",
    "                    torch.save(model.state_dict(), './tmp/sgns.epoch{}.batch{}'.format(epoch, batch_num))\n",
    "\n",
    "                i += 1\n",
    "                # 累积每个 batch 的 loss\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # 更新进度条并显示当前的 loss\n",
    "                pbar.set_postfix({'Epoch Loss': epoch_loss / (pbar.n + 1)})  # 计算并显示平均 loss\n",
    "                pbar.update(1)  # 更新进度条\n",
    "\n",
    "    # 训练结束，输出提示信息并返回模型\n",
    "    print(\"Optimization Finished!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 30000\n",
    "embedding_dim = 100\n",
    "epoch_num = 5\n",
    "batch_size = 32\n",
    "windows_size = 2\n",
    "neg_sample_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 58093/58093 [05:47<00:00, 167.07it/s, Epoch Loss=tensor(3.8627, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "Epoch 1: 100%|██████████| 58093/58093 [05:50<00:00, 165.73it/s, Epoch Loss=tensor(3.3761, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "Epoch 2: 100%|██████████| 58093/58093 [06:02<00:00, 160.21it/s, Epoch Loss=tensor(3.1653, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "Epoch 3: 100%|██████████| 58093/58093 [06:33<00:00, 147.52it/s, Epoch Loss=tensor(3.0202, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "Epoch 4: 100%|██████████| 58093/58093 [06:58<00:00, 138.92it/s, Epoch Loss=tensor(2.9088, device='cuda:0', grad_fn=<DivBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "model = train(train_data, vocabulary_size, embedding_dim, epoch_num, batch_size, windows_size, neg_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.target_embeddings.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于将词向量保存到文件中\n",
    "def save_embedding(embeds, file_name, id2word):\n",
    "    # 打开文件并写入词向量\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for idx in range(len(embeds)):\n",
    "            if idx in id2word:\n",
    "                word = id2word[idx]\n",
    "                embed = ' '.join(map(str, embeds[idx]))  # 将浮点数转换为字符串后再连接\n",
    "                f.write(word+' '+embed+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(word_embeddings, 'embeding/sgns.txt', index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine_Similarity_test(testpath, vocab, word_to_index, embeddings):\n",
    "    # 读取文件并解析每一行\n",
    "    with open(testpath, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # 初始化一个列表来存储每对子词的余弦相似度\n",
    "    similarity_list = []\n",
    "\n",
    "    # 遍历每一行\n",
    "    for line in lines:\n",
    "        # 分割每行中的两个子词\n",
    "        words = line.strip().split()\n",
    "        \n",
    "        # 检查两个子词是否都在word_embeddings中\n",
    "        if len(words) == 2 and words[0] in vocab and words[1] in vocab:\n",
    "            # 获取两个子词的词向量\n",
    "            vec1 = embeddings[word_to_index[words[0]]]\n",
    "            vec2 = embeddings[word_to_index[words[1]]]\n",
    "\n",
    "            # 归一化向量\n",
    "            vec1_normalized = vec1 / np.linalg.norm(vec1)\n",
    "            vec2_normalized = vec2 / np.linalg.norm(vec2)\n",
    "            \n",
    "            # 计算余弦相似度\n",
    "            dot_product = np.dot(vec1_normalized, vec2_normalized)\n",
    "\n",
    "            sim_svd = dot_product \n",
    "            \n",
    "            # 将余弦相似度添加到列表中\n",
    "            similarity_list.append(sim_svd)\n",
    "        else:\n",
    "            # 如果任一词向量不存在，设置相似度为0\n",
    "            similarity_list.append(0.0)\n",
    "\n",
    "    # # 打印或存储余弦相似度结果\n",
    "    # for words, sim in zip(lines, similarity_list):\n",
    "    #     print(f'Words: {words.strip()}, Cosine Similarity: {sim}')\n",
    "\n",
    "    with open('result/sgns.txt', 'w', encoding='utf-8') as file:\n",
    "        for words, sim in zip(lines, similarity_list):\n",
    "            str = f'Words: {words.strip()}, Cosine Similarity: {sim}' + '\\n'\n",
    "            file.write(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine_Similarity_test('pku_sim_test.txt', word_index, word_index, word_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
