{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import coo_matrix\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from zhon.hanzi import punctuation as pun\n",
    " \n",
    "# 将所有英文标点符号和中文标点符号合并为一个字符串\n",
    "allPun = string.punctuation + pun\n",
    " \n",
    "def delPunctuation(infile, outfile):\n",
    "    # 删除文本中的所有标点符号\n",
    "    with open(infile, 'r',encoding=\"utf-8\") as readFile, open(outfile, 'w', encoding=\"utf-8\") as writeFile:\n",
    "        for idx, line in enumerate(readFile):\n",
    "            # 将每行中的非标点符号字符连接起来\n",
    "            out = ''.join([i for i in line if i not in allPun])\n",
    "            writeFile.write(out)\n",
    "    readFile.close()\n",
    "    writeFile.close()\n",
    "\n",
    "# delPunctuation(\"training.txt\", \"ans.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取语料库文件\n",
    "def read_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # 读取文本并按行分割\n",
    "        sentences = file.readlines()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建共现矩阵\n",
    "def build_cooccurrence_matrix(sentences, window_size=5):\n",
    "    cooccurrence_matrix = defaultdict(int)\n",
    "\n",
    "    # 计算每个词语的频率和共现频率\n",
    "    for sentence in sentences:\n",
    "        words = sentence.strip().split()\n",
    "        for i, word in enumerate(words):\n",
    "            for j in range(i - window_size, i + window_size + 1):\n",
    "                if j >= 0 and j < len(words) and i != j:\n",
    "                    cooccurrence_matrix[word, words[j]] += 1\n",
    "\n",
    "    # vocab是包含所有唯一词语的集合\n",
    "    vocab = set(word for word_pair in cooccurrence_matrix.keys() for word in word_pair)\n",
    "\n",
    "    # 创建一个从词语到索引的映射\n",
    "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "    # 初始化一个COO格式的稀疏矩阵，其行数和列数等于词汇表的大小\n",
    "    cmatrix = coo_matrix((len(vocab), len(vocab)))\n",
    "\n",
    "    # 填充稀疏矩阵的值、行索引和列索引\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    for (i, j), value in cooccurrence_matrix.items():\n",
    "        row_index = word_to_index[i]\n",
    "        col_index = word_to_index[j]\n",
    "        rows.append(float(row_index))\n",
    "        cols.append(float(col_index))\n",
    "        data.append(float(value))\n",
    "\n",
    "    # 将行索引、列索引和数据列表赋值给COO格式的稀疏矩阵\n",
    "    cmatrix.data = np.array(data)\n",
    "    cmatrix.row = np.array(rows)\n",
    "    cmatrix.col = np.array(cols)\n",
    "\n",
    "    # 转换为CSR格式，这样可以进行更多的稀疏矩阵操作\n",
    "    csr_matrix = cmatrix.tocsr()\n",
    "    \n",
    "\n",
    "    return csr_matrix, vocab, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用SVD分解并降维\n",
    "def svd_embedding(csr_matrix, n_components=5):\n",
    "    # 使用SVD进行降维\n",
    "    U, sigma, Vt = svds(csr_matrix, k=n_components)\n",
    "    # 重新组合U和Vt来形成词向量\n",
    "    embeddings = np.dot(U, np.diag(sigma))\n",
    "    # 计算每个词向量的范数\n",
    "    scalar_array = np.linalg.norm(embeddings, axis=1)\n",
    "    # 对词向量进行归一化处理\n",
    "    embeddings = (embeddings.T / scalar_array).T\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine_Similarity_test(testpath, vocab, word_to_index, embeddings):\n",
    "    # 读取文件并解析每一行\n",
    "    with open(testpath, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # 初始化一个列表来存储每对子词的余弦相似度\n",
    "    similarity_list = []\n",
    "\n",
    "    # 遍历每一行\n",
    "    for line in lines:\n",
    "        # 分割每行中的两个子词\n",
    "        words = line.strip().split()\n",
    "        \n",
    "        # 检查两个子词是否都在word_embeddings中\n",
    "        if len(words) == 2 and words[0] in vocab and words[1] in vocab:\n",
    "            # 获取两个子词的词向量\n",
    "            vec1 = embeddings[word_to_index[words[0]]]\n",
    "            vec2 = embeddings[word_to_index[words[1]]]\n",
    "            \n",
    "            # 计算余弦相似度\n",
    "            dot_product = np.dot(vec1, vec2)\n",
    "            norm_vec1 = np.linalg.norm(vec1)\n",
    "            norm_vec2 = np.linalg.norm(vec2)\n",
    "            sim_svd = dot_product \n",
    "            \n",
    "            # 将余弦相似度添加到列表中\n",
    "            similarity_list.append(sim_svd)\n",
    "        else:\n",
    "            # 如果任一词向量不存在，设置相似度为0\n",
    "            similarity_list.append(0.0)\n",
    "\n",
    "    # # 打印或存储余弦相似度结果\n",
    "    # for words, sim in zip(lines, similarity_list):\n",
    "    #     print(f'Words: {words.strip()}, Cosine Similarity: {sim}')\n",
    "\n",
    "    with open('result/svd.txt', 'w', encoding='utf-8') as file:\n",
    "        for words, sim in zip(lines, similarity_list):\n",
    "            str = f'Words: {words.strip()}, Cosine Similarity: {sim}' + '\\n'\n",
    "            file.write(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取语料库\n",
    "sentences = read_corpus('training.txt')\n",
    "# 构建并应用SVD分解\n",
    "cooccurrence_matrix, vocab, word_to_index = build_cooccurrence_matrix(sentences, 5)\n",
    "word_embeddings = svd_embedding(cooccurrence_matrix, n_components=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine_Similarity_test('pku_sim_test.txt', vocab, word_to_index, word_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
